{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cabf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of send2trash: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    sys-platform (==\"darwin\") ; extra == 'objc'\n",
      "                 ~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9046a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/01 16:06:51 WARN Utils: Your hostname, liltimz resolves to a loopback address: 127.0.1.1; using 172.29.249.245 instead (on interface eth0)\n",
      "25/03/01 16:06:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.13.8, OpenJDK 64-Bit Server VM, 11.0.2\n",
      "Branch HEAD\n",
      "Compiled by user centos on 2023-06-19T22:21:01Z\n",
      "Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7853bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ee8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b8976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 16:54:49 WARN Utils: Your hostname, liltimz resolves to a loopback address: 127.0.1.1; using 172.29.249.245 instead (on interface eth0)\n",
      "25/03/07 16:54:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/07 16:55:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea305d",
   "metadata": {},
   "source": [
    "# create a dataframe with an array column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b488d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "| cd| [3, 4]|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create a dataframe with an array column\n",
    "df = spark.createDataFrame(\n",
    "        [(\"abc\", [1, 2]), (\"cd\", [3, 4])], [\"id\", \"numbers\"]\n",
    ") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f32dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- numbers: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrame to verify that the numbers column is an array.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c98a7078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:===================>                                       (1 + 2) / 3]\r",
      "\r",
      "[Stage 9:===========================================================(3 + 0) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|def| [3, 4]|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"abc\", [1, 2]), (\"def\", [3, 4])], [\"id\", \"numbers\"]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41127ea9",
   "metadata": {},
   "source": [
    "# creating DF with StructType syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25d71ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|  numbers|\n",
      "+---+---------+\n",
      "|  a|[1, 2, 3]|\n",
      "|  b|[4, 5, 6]|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating DF with StructType syntax\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"numbers\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "# Create sample data that matches the schema\n",
    "data = [\n",
    "    (\"a\", [1, 2, 3]),\n",
    "    (\"b\", [4, 5, 6])\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d66a56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652c4bc",
   "metadata": {},
   "source": [
    "# Add column \"first_number\" to the DF that returns the first element in the numbers array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f537baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------+\n",
      "| id|  numbers|first_number|\n",
      "+---+---------+------------+\n",
      "|  a|[1, 2, 3]|           1|\n",
      "|  b|[4, 5, 6]|           4|\n",
      "+---+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add column \"first_number\" to the DF that returns the first element in the numbers array\n",
    "# df.withColumn(\"first_number\", df.numbers[2]).show()\n",
    "# Note: withColumn function requires two arguments: the new column name and the column expression that defines its values\n",
    "df.withColumn(\"first_number\", col(\"numbers\")[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31eb08",
   "metadata": {},
   "source": [
    "# combine columns to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ff6e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|num1|num2|\n",
      "+----+----+\n",
      "|  33|  44|\n",
      "|  55|  66|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(33, 44), (55, 66)], [\"num1\", \"num2\"]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52cc75b",
   "metadata": {},
   "source": [
    "# Add another column \"nums\" which is an array that contains num1 and num2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a4d8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+\n",
      "|num1|num2|    nums|\n",
      "+----+----+--------+\n",
      "|  33|  44|[33, 44]|\n",
      "|  55|  66|[55, 66]|\n",
      "+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn(\"nums\", array(df.num1, df.num2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee310b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411160a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74187dc5",
   "metadata": {},
   "source": [
    "# LIST AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8bd2fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|first_name|color|\n",
      "+----------+-----+\n",
      "|       joe|  red|\n",
      "|       joe| blue|\n",
      "|       Tim|black|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DF with \"first_name\" and \"color\" columns that shows individuals and their colors\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(\"joe\", \"red\"), (\"joe\", \"blue\"), (\"Tim\", \"black\")], [\"first_name\", \"color\"]\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e3ffa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|first_name|     colors|\n",
      "+----------+-----------+\n",
      "|       joe|[red, blue]|\n",
      "|       Tim|    [black]|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Group by first_name, create an ArrayType column with all colors given first_name\n",
    "res = (df\n",
    "      .groupBy(df.first_name)\n",
    "      .agg(collect_list(col(\"color\")).alias(\"colors\")))\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27ece4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- colors: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.printSchema()\n",
    "# collect_list shows that some of Spark's API methods take advantage of ArrayType columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56833d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81fefdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|numbers|\n",
      "+---+-------+\n",
      "|abc| [1, 2]|\n",
      "|def| [3, 4]|\n",
      "|ghi| [5, 6]|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A PySpark array can be exploded into multiple rows, the opposite of collect_list.\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(\"abc\", [1, 2]), (\"def\", [3, 4]), (\"ghi\", [5, 6])], [\"id\", \"numbers\"]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb96b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the array column, so there is only one number per DataFrame row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd2eec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|number|\n",
      "+---+------+\n",
      "|abc|     1|\n",
      "|abc|     2|\n",
      "|def|     3|\n",
      "|def|     4|\n",
      "|ghi|     5|\n",
      "|ghi|     6|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"id\"), explode(col(\"numbers\")).alias(\"number\")).show()\n",
    "# collect_list collapses multiple rows into a single row. explode does the opposite and expands an array into multiple rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919504b",
   "metadata": {},
   "source": [
    "# Writing to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "756e93c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can write DataFrames with array columns to Parquet files without issue.\n",
    "# df = spark.createDataFrame(\n",
    "#     [(\"abc\", [1, 2]), (\"cd\", [3, 4])], [\"id\", \"numbers\"]\n",
    "# )\n",
    "\n",
    "# parquet_path = \"/Users/Liltimz/Desktop/slack/pyspark\"\n",
    "# df.write.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cb38c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You cannot write DataFrames with array columns to CSV files:\n",
    "# This isn't a limitation of Spark - it's a limitation of the CSV file format. CSV files can't handle complex column types like arrays. Parquet files are able to handle complex columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddaad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7554f8",
   "metadata": {},
   "source": [
    "# Type conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff350f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|letter|number|\n",
      "+------+------+\n",
      "|     a|     8|\n",
      "|     b|     9|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with an integer column and a string column to demonstrate the surprising type conversion that takes place when different types are combined in a PySpark array\n",
    "df = spark.createDataFrame(\n",
    "    [(\"a\", 8), (\"b\", 9)], [\"letter\", \"number\"]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2cf6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+\n",
      "|letter|number|   arr|number2|\n",
      "+------+------+------+-------+\n",
      "|     a|     8|[a, 8]|      8|\n",
      "|     b|     9|[b, 9]|      9|\n",
      "+------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the letter and number columns into an array \"arr\" and then fetch the number from the array\n",
    "res = (df\n",
    "       .withColumn(\"arr\", array(df.letter, df.number))\n",
    "       .withColumn(\"number2\", col(\"arr\")[1]))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da6d12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- letter: string (nullable = true)\n",
      " |-- number: long (nullable = true)\n",
      " |-- arr: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- number2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to observe the number2 column is string type.\n",
    "res.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abccb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Python lists can hold values with different types. my_arr = [1, \"a\"] is valid in Python.\n",
    "\n",
    "# PySpark arrays can only hold one type. In order to combine letter and number in an array, PySpark needs to convert number to a string.\n",
    "\n",
    "# PySpark's type conversion causes you to lose valuable type information. It's arguable that the array function should error out when joining columns with different types, rather than implicitly converting types.\n",
    "\n",
    "# It's best for you to explicitly convert types when combining different types into a PySpark array rather than relying on implicit conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1803380f",
   "metadata": {},
   "source": [
    "# Avoiding Dots / Periods in PySpark Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df71466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|country.name|   continent|\n",
      "+------------+------------+\n",
      "|       china|        asia|\n",
      "|    colombia|souh america|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create a DataFrame with country.name and continent columns.\n",
    "df = spark.createDataFrame(\n",
    "    [(\"china\", \"asia\"), (\"colombia\", \"souh america\")], [\"country.name\", \"continent\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7875ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(\"country.name\")\n",
    "# this will give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ea6e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|person.name|     person|\n",
      "+-----------+-----------+\n",
      "|    charles|{chuck, 42}|\n",
      "|   lawrence|{larry, 73}|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"person.name\", StringType(), True),\n",
    "    StructField(\"person\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True)]))\n",
    "])\n",
    "data = [\n",
    "    (\"charles\", Row(\"chuck\", 42)),\n",
    "    (\"lawrence\", Row(\"larry\", 73))\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb5c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+\n",
      "|     person| name|person.name|\n",
      "+-----------+-----+-----------+\n",
      "|{chuck, 42}|chuck|    charles|\n",
      "|{larry, 73}|larry|   lawrence|\n",
      "+-----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\"person\", \"person.name\", \"`person.name`\"]\n",
    "df.select(cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "788d09e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|person_name|     person|\n",
      "+-----------+-----------+\n",
      "|    charles|{chuck, 42}|\n",
      "|   lawrence|{larry, 73}|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:===================>                                       (1 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Replaces dots with underscores in column names\n",
    "clean_df = df.toDF(*(c.replace('.', '_') for c in df.columns))\n",
    "clean_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4135d1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---+\n",
      "|person_name| name|age|\n",
      "+-----------+-----+---+\n",
      "|    charles|chuck| 42|\n",
      "|   lawrence|larry| 73|\n",
      "+-----------+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:===================>                                       (1 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clean_df.select(\"person_name\", \"person.name\", \"person.age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fca340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
